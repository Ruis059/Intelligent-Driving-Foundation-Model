# Intelligent-Driving-Foundation-Model

### 1. 辅助决策与规划（Decision and Planning）

<table>
<thead>
  <tr>
    <th>标题</th>
    <th>论文</th>
    <th>代码</th>
    <th>发表期刊或会议</th>
    <th>年份</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Drivegpt4: Interpretable end-to-end autonomous driving via large language model</td>
    <td><a href="https://ieeexplore.ieee.org/abstract/document/10629039">Paper</a></td>
    <td><a href="https://cloud.tsinghua.edu.cn/d/adbc8fa3a2fc420ca7bc/">Code</a></td>
    <td>IEEE Robotics and Automation Letters</td>
    <td>2024</td>
  </tr>
  <tr>
    <td>DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving</td>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.html">Paper</a></td>
    <td></td>
    <td>CVPR</td>
    <td>2025</td>
  </tr>
  <tr>
    <td>Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model</td>
    <td><a href="https://arxiv.org/abs/2402.10828">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2024</td>
  </tr>  
  <tr>
    <td>ChatGPT as your vehicle co-pilot: An initial attempt</td>
    <td><a href="https://ieeexplore.ieee.org/abstract/document/10286969">Paper</a></td>
    <td></td>
    <td>IEEE Transactions on Intelligent Vehicles</td>
    <td>2023</td>
  </tr> 
  <tr>
    <td>Simlingo: Vision-only closed-loop autonomous driving with language-action alignment</td>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment_CVPR_2025_paper.html">Paper</a></td>
    <td><a href="https://github.com/RenzKa/simlingo?tab=readme-ov-file#training">Code</a></td>
    <td>CVPR</td>
    <td>2025</td>
  </tr>  
  <tr>
    <td>Mtd-gpt: A multi-task decision-making gpt model for autonomous driving at unsignalized intersections</td>
    <td><a href="https://ieeexplore.ieee.org/abstract/document/10421993">Paper</a></td>
    <td></td>
    <td>ITSC</td>
    <td>2023</td>
  </tr>  
  <tr>
    <td>A language agent for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2311.10813">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2023</td>
  </tr>  
  <tr>
    <td>Drivelm: Driving with graph visual question answering</td>
    <td><a href="https://link.springer.com/chapter/10.1007/978-3-031-72943-0_15">Paper</a></td>
    <td><a href="https://github.com/OpenDriveLab/DriveLM">Code</a></td>
    <td>ECCV</td>
    <td>2024</td>
  </tr>  
  <tr>
    <td>Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2312.09245">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2023</td>
  </tr>  
  <tr>
    <td>Vlp: Vision language planning for autonomous driving</td>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pan_VLP_Vision_Language_Planning_for_Autonomous_Driving_CVPR_2024_paper.html">Paper</a></td>
    <td></td>
    <td>CVPR</td>
    <td>2024</td>
  </tr> 
  <tr>
    <td>Dme-driver: Integrating human decision logic and 3d scene perception in autonomous driving</td>
    <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/32346">Paper</a></td>
    <td></td>
    <td>AAAI</td>
    <td>2025</td>
  </tr>  
  <tr>
    <td>Emma: end-to-end multimodal model for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2410.23262">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2024</td>
  </tr>
  <tr>
    <td>Agentscodriver: Large language model empowered collaborative driving with lifelong learning</td>
    <td><a href="https://arxiv.org/abs/2404.06345">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2024</td>
  </tr>
  <tr>
    <td>Langcoop: Collaborative driving with language</td>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025W/MEIS/html/Gao_LangCoop_Collaborative_Driving_with_Language_CVPRW_2025_paper.html">Paper</a></td>
    <td><a href="https://github.com/taco-group/LangCoop">Code</a></td>
    <td>CVPR Workshop</td>
    <td>2025</td>
  </tr>  
  <tr>
    <td>DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving</td>
    <td><a href="https://arxiv.org/abs/2505.16278">Paper</a></td>
    <td><a href="https://github.com/Thinklab-SJTU/DriveMoE">Code</a></td>
    <td>ArXiv</td>
    <td>2025</td>
  </tr>  
  <tr>
    <td>World4drive: End-to-end autonomous driving via intention-aware physical latent world model</td>
    <td><a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_World4Drive_End-to-End_Autonomous_Driving_via_Intention-aware_Physical_Latent_World_Model_ICCV_2025_paper.html">Paper</a></td>
    <td><a href="https://github.com/ucaszyp/World4Drive">Code</a></td>
    <td>ICCV</td>
    <td>2025</td>
  </tr>
</tbody>
</table>

### 2. 环境感知与运动预测（Perception and Motion Prediction）

<table>
<thead>
  <tr>
    <th>标题</th>
    <th>论文</th>
    <th>代码</th>
    <th>发表期刊或会议</th>
    <th>年份</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Hilm-d: Towards high-resolution understanding in multimodal large language models for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2309.05186">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2023</td>
  </tr>  
  <tr>
    <td>Safeauto: Knowledge-enhanced safe autonomous driving with multimodal foundation models</td>
    <td><a href="https://arxiv.org/abs/2503.00211">Paper</a></td>
    <td><a href="https://github.com/AI-secure/SafeAuto">Code</a></td>
    <td>ICML</td>
    <td>2025</td>
  </tr>
  <tr>
    <td>Language prompt for autonomous driving</td>
    <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/32902">Paper</a></td>
    <td></td>
    <td>AAAI</td>
    <td>2025</td>
  </tr>  
  <tr>
    <td>Lc-llm: Explainable lane-change intention and trajectory predictions with large language models</td>
    <td><a href="https://www.sciencedirect.com/science/article/pii/S2772424725000101">Paper</a></td>
    <td></td>
    <td>Communications in Transportation Research</td>
    <td>##</td>
  </tr>
  <tr>
    <td>Diffvla: Vision-language guided diffusion planning for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2505.19381">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2025</td>
  </tr>  
</tbody>
</table>

### 3. 视觉问答（Visual Question Answering, VQA）与可解释交互

<table>
<thead>
  <tr>
    <th>标题</th>
    <th>论文</th>
    <th>代码</th>
    <th>发表期刊或会议</th>
    <th>年份</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Gpt-driver: Learning to drive with gpt</td>
    <td><a href="https://arxiv.org/abs/2310.01415">Paper</a></td>
    <td><a href="https://github.com/PointsCoder/GPT-Driver">Code</a></td>
    <td>ArXiv</td>
    <td>2023</td>
  </tr>
  <tr>
    <td>Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model</td>
    <td><a href="https://arxiv.org/abs/2402.10828">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2024</td> 
  </tr>
  <tr>
    <td>Covla: Comprehensive vision-language-action dataset for autonomous driving</td>
    <td><a href="https://ieeexplore.ieee.org/abstract/document/10944039/">Paper</a></td>
    <td><a href="https://turingmotors.github.io/covla-ad/">Code</a></td>
    <td>WACV</td>
    <td>2025</td>
  </tr>
  <tr>
    <td>Drivecot: Integrating chain-of-thought reasoning with end-to-end driving</td>
    <td><a href="https://arxiv.org/abs/2403.16996">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2024</td> 
  </tr>  
  <tr>
    <td>Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving</td>
    <td><a href="https://link.springer.com/chapter/10.1007/978-3-031-73347-5_17">Paper</a></td>
    <td><a href="https://github.com/fudan-zvg/reason2drive">Code</a></td>
    <td>ECCV</td>
    <td>2024</td>
  </tr>
  <tr>
    <td>Futuresightdrive: Thinking visually with spatio-temporal cot for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2505.17685">Paper</a></td>
    <td><a href="https://github.com/MIV-XJTU/FSDrive">Code</a></td>
    <td>NeurIPS</td>
    <td>2025</td> 
  </tr>  
  <tr>
    <td>Cot-drive: Efficient motion forecasting for autonomous driving with llms and chain-of-thought prompting</td>
    <td><a href="https://ieeexplore.ieee.org/abstract/document/10980428">Paper</a></td>
    <td><a href="##">Code</a></td>
    <td>IEEE Transactions on Artificial Intelligence</td>
    <td>2025</td>
  </tr>
  <tr>
    <td>Recogdrive: A reinforced cognitive framework for end-to-end autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2506.08052">Paper</a></td>
    <td><a href="https://github.com/xiaomi-research/recogdrive">Code</a></td>
    <td>ICLR</td>
    <td>2026</td> 
  </tr>
</tbody>
</table>

### 4. 数据生成与世界模型（Data Generation and World Model）

<table>
<thead>
  <tr>
    <th>标题</th>
    <th>论文</th>
    <th>代码</th>
    <th>发表期刊或会议</th>
    <th>年份</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Model with Spatio-Temporal Visual Representation</td>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xie_S4-Driver_Scalable_Self-Supervised_Driving_Multimodal_Large_Language_Model_with_Spatio-Temporal_CVPR_2025_paper.html">Paper</a></td>
    <td></td>
    <td>CVPR</td>
    <td>2025</td> 
  </tr>  <tr>
    <td>DrivingDiffusion: layout-guided multi-view driving scenarios video generation with latent diffusion model</td>
    <td><a href="https://link.springer.com/chapter/10.1007/978-3-031-73229-4_27">Paper</a></td>
    <td><a href="https://github.com/shalfun/DrivingDiffusion">Code</a></td>
    <td>ECCV</td>
    <td>2024</td>
  </tr>
  <tr>
    <td>Gaia-1: A generative world model for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2309.17080">Paper</a></td>
    <td></td>
    <td>ArXiv</td>
    <td>2023</td> 
  </tr>  <tr>
    <td>Opendrivevla: Towards end-to-end autonomous driving with large vision language action model</td>
    <td><a href="https://arxiv.org/abs/2503.23463">Paper</a></td>
    <td><a href="https://github.com/DriveVLA/OpenDriveVLA">Code</a></td>
    <td>ArXiv</td>
    <td>2025</td>
  </tr>
  <tr>
    <td>Adriver-i: A general world model for autonomous driving</td>
    <td><a href="https://arxiv.org/abs/2311.13549">Paper</a></td>
    <td><a href="##">Code</a></td>
    <td>ArXiv</td>
    <td>2023</td> 
  </tr>
</tbody>
</table>
